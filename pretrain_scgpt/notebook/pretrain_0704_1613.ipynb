{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model with Batch Integration\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the batch integration task. We use the PBMC 10K dataset as an example and fine-tune on the pre-trained whole-body model. \n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on integration tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libjpeg.so.8: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../\")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for integration task\n",
    "Here we provide some hyper-parameter recommendations here for the integration task. Note that the PBMC 10K dataset contains multiple batches to be integrated. Therefore, in addition to the default gene modelling objectives, we also turn on ESC, DAR and DSBN objectives specifically to faciliate batch integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqiliu-ghddi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_scgpt/notebook/wandb/run-20230704_173251-p6q7wsnl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qiliu-ghddi/scGPT/runs/p6q7wsnl' target=\"_blank\">blooming-disco-8</a></strong> to <a href='https://wandb.ai/qiliu-ghddi/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qiliu-ghddi/scGPT' target=\"_blank\">https://wandb.ai/qiliu-ghddi/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qiliu-ghddi/scGPT/runs/p6q7wsnl' target=\"_blank\">https://wandb.ai/qiliu-ghddi/scGPT/runs/p6q7wsnl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'PBMC_10K', 'do_train': True, 'load_model': '../save/scGPT_human', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 16, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=42,\n",
    "    dataset_name=\"PBMC_10K\", # Dataset name\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../save/scGPT_human\", # Path to pre-trained model\n",
    "    GEPC=True,  # Gene expression modelling for cell objective\n",
    "    ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0, # DAR objective weight for batch correction\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    epochs=15, # Default number of epochs for fine-tuning\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    lr=1e-4, # Default learning rate for fine-tuning\n",
    "    batch_size=16, # Default batch size for fine-tuning\n",
    "    layer_size=128,\n",
    "    nlayers=4,\n",
    "    nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "    save_eval_interval=5, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=True, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # # Default setting: Automatic Mixed Precision\n",
    ")\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = 1200  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = True\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_PBMC_10K-Jul04-17-32\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the PBMC 10K data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File data/gene_info_pbmc.csv already downloaded                                     \n",
      "\u001b[34mINFO    \u001b[0m File data/pbmc_metadata.pickle already downloaded                                   \n",
      "\u001b[34mINFO    \u001b[0m File data/pbmc8k/filtered_gene_bc_matrices.tar.gz already downloaded                \n",
      "\u001b[34mINFO    \u001b[0m Extracting tar file                                                                 \n",
      "\u001b[34mINFO    \u001b[0m Removing extracted data at data/pbmc8k/filtered_gene_bc_matrices                    \n",
      "\u001b[34mINFO    \u001b[0m File data/pbmc4k/filtered_gene_bc_matrices.tar.gz already downloaded                \n",
      "\u001b[34mINFO    \u001b[0m Extracting tar file                                                                 \n",
      "\u001b[34mINFO    \u001b[0m Removing extracted data at data/pbmc4k/filtered_gene_bc_matrices                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/anndata/_core/anndata.py:1785: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  [AnnData(sparse.csr_matrix(a.shape), obs=a.obs) for a in all_adatas],\n",
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/scvi/data/_built_in_data/_pbmc.py:81: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  barcodes_metadata = pbmc_metadata[\"barcodes\"].index.values.ravel().astype(np.str)\n",
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/scvi/data/_built_in_data/_pbmc.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  [not barcode.endswith(\"11\") for barcode in barcodes_metadata], dtype=np.bool\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == \"PBMC_10K\":\n",
    "    adata = scvi.data.pbmc_dataset()  # 11990 × 3346\n",
    "    ori_batch_col = \"batch\"\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"str_labels\"].astype(\"category\")\n",
    "    adata.var = adata.var.set_index(\"gene_symbols\")\n",
    "    data_is_raw = True\n",
    "\n",
    "# make the batch category column\n",
    "adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cross-check gene set with the pre-trained model \n",
    "Note that we retain the common gene set between the data and the pre-trained model for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'PBMC_10K', 'do_train': True, 'load_model': '../save/scGPT_human', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 16, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "config.load_model = None\n",
    "print(config.load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Pretraining\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "    \n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    print(\"... Pretraining\")\n",
    "    embsize = config.layer_size \n",
    "    nhead = config.nhead\n",
    "    nlayers = config.nlayers  \n",
    "    d_hid = config.layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pre-process the data\n",
    "We follow the standardized pipline of depth normalization, log normalization, and highly vairable gene (HVG) selection for data pre-processing. We further introduced value binning to obtain the relative expressions of each HVG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"str_batch\" if dataset_name != \"heart_cell\" else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 11990 × 1200\n",
      "    obs: 'n_counts', 'batch', 'labels', 'str_labels', 'celltype', 'str_batch', 'batch_id'\n",
      "    var: 'n_counts-0', 'n_counts-1', 'n_counts', 'gene_name', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'highly_variable_nbatches'\n",
      "    uns: 'cell_types', 'log1p', 'hvg'\n",
      "    obsm: 'design', 'raw_qc', 'normalized_qc', 'qc_pc', 'bin_edges'\n",
      "    layers: 'X_normed', 'X_log1p', 'X_binned'\n"
     ]
    }
   ],
   "source": [
    "print(adata_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 10791, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 1199, \n",
      "\t feature length: 1201\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 需要改的参数\n",
    "\n",
    "DSBN = False\n",
    "config.load_model = None\n",
    "config.fast_transformer = False\n",
    "config.GEPC = False \n",
    "\n",
    "do_dab = False\n",
    "use_batch_labels = False\n",
    "print(config.fast_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple batchnorm instead of domain specific batchnorm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=do_dab,\n",
    "    use_batch_labels=use_batch_labels,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "                \n",
    "#             if config.GEPC:\n",
    "#                 loss_gepc = criterion(\n",
    "#                     output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "#                 )\n",
    "#                 loss = loss + loss_gepc\n",
    "#                 metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "#             if config.GEPC and explicit_zero_prob:\n",
    "#                 loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "#                     output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "#                 )\n",
    "#                 loss = loss + loss_gepc_zero_log_prob\n",
    "#                 metrics_to_log.update(\n",
    "#                     {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "#                 )\n",
    "                \n",
    "#             if config.ecs_thres > 0:\n",
    "#                 loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "#                 loss = loss + loss_ecs\n",
    "#                 metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            # loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            # loss = loss + config.dab_weight * loss_dab\n",
    "            # metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                # loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            # total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "#     wandb.log(\n",
    "#         {\n",
    "#             \"valid/mse\": total_loss / total_num,\n",
    "#             \"valid/mre\": total_error / total_num,\n",
    "#             \"valid/dab\": total_dab / total_num,\n",
    "#             \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "#             / total_num,\n",
    "#             \"epoch\": epoch,\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"str_batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"celltype\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/675 batches | lr 0.0001 | ms/batch 74.85 | loss 101.33 | mse 100.82 | mre 1675113.52 |\n",
      "scGPT - INFO - | epoch   1 | 200/675 batches | lr 0.0001 | ms/batch 67.32 | loss 83.53 | mse 83.12 | mre 1938796.12 |\n",
      "scGPT - INFO - | epoch   1 | 300/675 batches | lr 0.0001 | ms/batch 67.69 | loss 65.40 | mse 65.02 | mre 1790400.67 |\n",
      "scGPT - INFO - | epoch   1 | 400/675 batches | lr 0.0001 | ms/batch 67.18 | loss 59.94 | mse 59.58 | mre 1798798.61 |\n",
      "scGPT - INFO - | epoch   1 | 500/675 batches | lr 0.0001 | ms/batch 67.22 | loss 58.38 | mse 58.03 | mre 1748725.18 |\n",
      "scGPT - INFO - | epoch   1 | 600/675 batches | lr 0.0001 | ms/batch 67.68 | loss 56.55 | mse 56.22 | mre 1696328.85 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 48.27s | valid loss/mse 56.4876 | mre 1626001.3144\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 56.4876\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/675 batches | lr 0.0001 | ms/batch 68.12 | loss 57.69 | mse 57.37 | mre 1737179.79 |\n",
      "scGPT - INFO - | epoch   2 | 200/675 batches | lr 0.0001 | ms/batch 67.80 | loss 56.87 | mse 56.55 | mre 1703112.00 |\n",
      "scGPT - INFO - | epoch   2 | 300/675 batches | lr 0.0001 | ms/batch 67.41 | loss 55.71 | mse 55.40 | mre 1693840.35 |\n",
      "scGPT - INFO - | epoch   2 | 400/675 batches | lr 0.0001 | ms/batch 67.64 | loss 55.93 | mse 55.63 | mre 1709789.44 |\n",
      "scGPT - INFO - | epoch   2 | 500/675 batches | lr 0.0001 | ms/batch 67.40 | loss 54.42 | mse 54.13 | mre 1677251.78 |\n",
      "scGPT - INFO - | epoch   2 | 600/675 batches | lr 0.0001 | ms/batch 67.44 | loss 52.80 | mse 52.51 | mre 1607660.59 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 47.63s | valid loss/mse 54.2788 | mre 1543360.0132\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 54.2788\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/675 batches | lr 0.0001 | ms/batch 70.95 | loss 55.77 | mse 55.48 | mre 1679556.41 |\n",
      "scGPT - INFO - | epoch   3 | 200/675 batches | lr 0.0001 | ms/batch 67.51 | loss 54.31 | mse 54.03 | mre 1668112.88 |\n",
      "scGPT - INFO - | epoch   3 | 300/675 batches | lr 0.0001 | ms/batch 67.44 | loss 53.52 | mse 53.24 | mre 1641180.47 |\n",
      "scGPT - INFO - | epoch   3 | 400/675 batches | lr 0.0001 | ms/batch 67.40 | loss 52.99 | mse 52.71 | mre 1628226.10 |\n",
      "scGPT - INFO - | epoch   3 | 500/675 batches | lr 0.0001 | ms/batch 68.61 | loss 51.92 | mse 51.64 | mre 1576992.51 |\n",
      "scGPT - INFO - | epoch   3 | 600/675 batches | lr 0.0001 | ms/batch 69.49 | loss 51.00 | mse 50.73 | mre 1574296.65 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 48.26s | valid loss/mse 51.5935 | mre 1625660.2869\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 51.5935\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/675 batches | lr 0.0001 | ms/batch 68.11 | loss 53.35 | mse 53.07 | mre 1637767.48 |\n",
      "scGPT - INFO - | epoch   4 | 200/675 batches | lr 0.0001 | ms/batch 67.40 | loss 51.66 | mse 51.39 | mre 1588464.52 |\n",
      "scGPT - INFO - | epoch   4 | 300/675 batches | lr 0.0001 | ms/batch 67.52 | loss 52.15 | mse 51.88 | mre 1588720.38 |\n",
      "scGPT - INFO - | epoch   4 | 400/675 batches | lr 0.0001 | ms/batch 67.33 | loss 52.07 | mse 51.80 | mre 1610012.59 |\n",
      "scGPT - INFO - | epoch   4 | 500/675 batches | lr 0.0001 | ms/batch 67.69 | loss 50.86 | mse 50.59 | mre 1561107.85 |\n",
      "scGPT - INFO - | epoch   4 | 600/675 batches | lr 0.0001 | ms/batch 67.45 | loss 50.45 | mse 50.19 | mre 1549341.01 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 47.62s | valid loss/mse 51.1120 | mre 1555069.1349\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 51.1120\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/675 batches | lr 0.0001 | ms/batch 68.04 | loss 52.68 | mse 52.41 | mre 1617594.18 |\n",
      "scGPT - INFO - | epoch   5 | 200/675 batches | lr 0.0001 | ms/batch 67.91 | loss 51.97 | mse 51.70 | mre 1599361.26 |\n",
      "scGPT - INFO - | epoch   5 | 300/675 batches | lr 0.0001 | ms/batch 67.57 | loss 51.25 | mse 50.98 | mre 1562963.63 |\n",
      "scGPT - INFO - | epoch   5 | 400/675 batches | lr 0.0001 | ms/batch 67.34 | loss 52.12 | mse 51.85 | mre 1606422.95 |\n",
      "scGPT - INFO - | epoch   5 | 500/675 batches | lr 0.0001 | ms/batch 67.45 | loss 50.77 | mse 50.50 | mre 1558323.21 |\n",
      "scGPT - INFO - | epoch   5 | 600/675 batches | lr 0.0001 | ms/batch 67.45 | loss 49.75 | mse 49.50 | mre 1537756.03 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 47.65s | valid loss/mse 51.0635 | mre 1629631.8145\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 51.0635\n",
      "scGPT - INFO - Saving model to save/dev_PBMC_10K-Jul04-17-32\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/675 batches | lr 0.0001 | ms/batch 68.13 | loss 52.04 | mse 51.77 | mre 1599868.75 |\n",
      "scGPT - INFO - | epoch   6 | 200/675 batches | lr 0.0001 | ms/batch 67.33 | loss 51.32 | mse 51.06 | mre 1580795.09 |\n",
      "scGPT - INFO - | epoch   6 | 300/675 batches | lr 0.0001 | ms/batch 67.18 | loss 51.17 | mse 50.90 | mre 1563023.93 |\n",
      "scGPT - INFO - | epoch   6 | 400/675 batches | lr 0.0001 | ms/batch 67.25 | loss 51.49 | mse 51.23 | mre 1594346.08 |\n",
      "scGPT - INFO - | epoch   6 | 500/675 batches | lr 0.0001 | ms/batch 67.28 | loss 50.06 | mse 49.80 | mre 1535673.19 |\n",
      "scGPT - INFO - | epoch   6 | 600/675 batches | lr 0.0001 | ms/batch 67.33 | loss 49.31 | mse 49.06 | mre 1534725.05 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 47.55s | valid loss/mse 50.4046 | mre 1660832.4222\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 50.4046\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/675 batches | lr 0.0001 | ms/batch 67.97 | loss 51.23 | mse 50.96 | mre 1570232.96 |\n",
      "scGPT - INFO - | epoch   7 | 200/675 batches | lr 0.0001 | ms/batch 67.26 | loss 50.33 | mse 50.07 | mre 1551221.35 |\n",
      "scGPT - INFO - | epoch   7 | 300/675 batches | lr 0.0001 | ms/batch 67.16 | loss 50.29 | mse 50.03 | mre 1552364.48 |\n",
      "scGPT - INFO - | epoch   7 | 400/675 batches | lr 0.0001 | ms/batch 67.29 | loss 50.05 | mse 49.78 | mre 1544450.67 |\n",
      "scGPT - INFO - | epoch   7 | 500/675 batches | lr 0.0001 | ms/batch 67.36 | loss 50.00 | mse 49.74 | mre 1551379.49 |\n",
      "scGPT - INFO - | epoch   7 | 600/675 batches | lr 0.0001 | ms/batch 67.47 | loss 49.01 | mse 48.75 | mre 1509832.30 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 47.51s | valid loss/mse 49.3238 | mre 1481503.0981\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 49.3238\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/675 batches | lr 0.0000 | ms/batch 68.07 | loss 50.81 | mse 50.54 | mre 1560035.94 |\n",
      "scGPT - INFO - | epoch   8 | 200/675 batches | lr 0.0000 | ms/batch 67.39 | loss 50.45 | mse 50.19 | mre 1552003.83 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   8 | 300/675 batches | lr 0.0000 | ms/batch 67.68 | loss 49.67 | mse 49.41 | mre 1539974.46 |\n",
      "scGPT - INFO - | epoch   8 | 400/675 batches | lr 0.0000 | ms/batch 67.33 | loss 49.53 | mse 49.27 | mre 1534123.27 |\n",
      "scGPT - INFO - | epoch   8 | 500/675 batches | lr 0.0000 | ms/batch 67.33 | loss 48.41 | mse 48.16 | mre 1509818.26 |\n",
      "scGPT - INFO - | epoch   8 | 600/675 batches | lr 0.0000 | ms/batch 67.48 | loss 48.02 | mse 47.76 | mre 1480818.40 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 47.57s | valid loss/mse 48.9129 | mre 1596678.5166\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 48.9129\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/675 batches | lr 0.0000 | ms/batch 68.25 | loss 50.07 | mse 49.81 | mre 1543330.82 |\n",
      "scGPT - INFO - | epoch   9 | 200/675 batches | lr 0.0000 | ms/batch 67.30 | loss 49.98 | mse 49.71 | mre 1557972.09 |\n",
      "scGPT - INFO - | epoch   9 | 300/675 batches | lr 0.0000 | ms/batch 67.26 | loss 49.46 | mse 49.20 | mre 1520789.28 |\n",
      "scGPT - INFO - | epoch   9 | 400/675 batches | lr 0.0000 | ms/batch 67.26 | loss 49.27 | mse 49.01 | mre 1530736.17 |\n",
      "scGPT - INFO - | epoch   9 | 500/675 batches | lr 0.0000 | ms/batch 67.32 | loss 48.99 | mse 48.73 | mre 1527082.00 |\n",
      "scGPT - INFO - | epoch   9 | 600/675 batches | lr 0.0000 | ms/batch 67.81 | loss 47.72 | mse 47.47 | mre 1473211.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 47.58s | valid loss/mse 48.8848 | mre 1433407.8988\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 48.8848\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/675 batches | lr 0.0000 | ms/batch 67.94 | loss 49.58 | mse 49.32 | mre 1535814.30 |\n",
      "scGPT - INFO - | epoch  10 | 200/675 batches | lr 0.0000 | ms/batch 67.30 | loss 50.22 | mse 49.96 | mre 1543481.06 |\n",
      "scGPT - INFO - | epoch  10 | 300/675 batches | lr 0.0000 | ms/batch 67.33 | loss 49.25 | mse 48.99 | mre 1528847.47 |\n",
      "scGPT - INFO - | epoch  10 | 400/675 batches | lr 0.0000 | ms/batch 67.34 | loss 48.56 | mse 48.30 | mre 1497159.80 |\n",
      "scGPT - INFO - | epoch  10 | 500/675 batches | lr 0.0000 | ms/batch 67.25 | loss 48.84 | mse 48.59 | mre 1517552.63 |\n",
      "scGPT - INFO - | epoch  10 | 600/675 batches | lr 0.0000 | ms/batch 67.42 | loss 47.42 | mse 47.17 | mre 1477262.04 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 47.51s | valid loss/mse 48.9108 | mre 1573446.6502\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_PBMC_10K-Jul04-17-32\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/675 batches | lr 0.0000 | ms/batch 68.39 | loss 49.40 | mse 49.13 | mre 1538359.97 |\n",
      "scGPT - INFO - | epoch  11 | 200/675 batches | lr 0.0000 | ms/batch 67.94 | loss 49.34 | mse 49.08 | mre 1528756.10 |\n",
      "scGPT - INFO - | epoch  11 | 300/675 batches | lr 0.0000 | ms/batch 67.95 | loss 49.49 | mse 49.23 | mre 1532206.49 |\n",
      "scGPT - INFO - | epoch  11 | 400/675 batches | lr 0.0000 | ms/batch 67.38 | loss 49.41 | mse 49.15 | mre 1526429.33 |\n",
      "scGPT - INFO - | epoch  11 | 500/675 batches | lr 0.0000 | ms/batch 67.60 | loss 47.77 | mse 47.52 | mre 1480142.61 |\n",
      "scGPT - INFO - | epoch  11 | 600/675 batches | lr 0.0000 | ms/batch 67.35 | loss 47.65 | mse 47.40 | mre 1492852.03 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 47.71s | valid loss/mse 48.5675 | mre 1450886.9597\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 48.5675\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/675 batches | lr 0.0000 | ms/batch 68.54 | loss 49.22 | mse 48.96 | mre 1523717.12 |\n",
      "scGPT - INFO - | epoch  12 | 200/675 batches | lr 0.0000 | ms/batch 67.26 | loss 49.12 | mse 48.87 | mre 1514060.53 |\n",
      "scGPT - INFO - | epoch  12 | 300/675 batches | lr 0.0000 | ms/batch 67.33 | loss 48.86 | mse 48.60 | mre 1511550.01 |\n",
      "scGPT - INFO - | epoch  12 | 400/675 batches | lr 0.0000 | ms/batch 67.19 | loss 49.38 | mse 49.12 | mre 1535966.76 |\n",
      "scGPT - INFO - | epoch  12 | 500/675 batches | lr 0.0000 | ms/batch 67.47 | loss 48.14 | mse 47.89 | mre 1499467.36 |\n",
      "scGPT - INFO - | epoch  12 | 600/675 batches | lr 0.0000 | ms/batch 67.70 | loss 48.15 | mse 47.90 | mre 1503493.57 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 47.61s | valid loss/mse 48.2550 | mre 1511302.8842\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 48.2550\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/675 batches | lr 0.0000 | ms/batch 68.08 | loss 49.36 | mse 49.10 | mre 1523235.42 |\n",
      "scGPT - INFO - | epoch  13 | 200/675 batches | lr 0.0000 | ms/batch 67.44 | loss 49.33 | mse 49.07 | mre 1527159.68 |\n",
      "scGPT - INFO - | epoch  13 | 300/675 batches | lr 0.0000 | ms/batch 67.36 | loss 49.08 | mse 48.82 | mre 1517295.30 |\n",
      "scGPT - INFO - | epoch  13 | 400/675 batches | lr 0.0000 | ms/batch 67.34 | loss 48.58 | mse 48.33 | mre 1512779.62 |\n",
      "scGPT - INFO - | epoch  13 | 500/675 batches | lr 0.0000 | ms/batch 67.39 | loss 48.69 | mse 48.44 | mre 1513842.18 |\n",
      "scGPT - INFO - | epoch  13 | 600/675 batches | lr 0.0000 | ms/batch 67.42 | loss 47.39 | mse 47.14 | mre 1467151.82 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 47.57s | valid loss/mse 48.6096 | mre 1538774.9063\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/675 batches | lr 0.0000 | ms/batch 68.20 | loss 49.49 | mse 49.23 | mre 1520256.90 |\n",
      "scGPT - INFO - | epoch  14 | 200/675 batches | lr 0.0000 | ms/batch 67.58 | loss 48.70 | mse 48.44 | mre 1519431.41 |\n",
      "scGPT - INFO - | epoch  14 | 300/675 batches | lr 0.0000 | ms/batch 67.82 | loss 49.10 | mse 48.85 | mre 1533184.03 |\n",
      "scGPT - INFO - | epoch  14 | 400/675 batches | lr 0.0000 | ms/batch 67.39 | loss 49.10 | mse 48.85 | mre 1521471.99 |\n",
      "scGPT - INFO - | epoch  14 | 500/675 batches | lr 0.0000 | ms/batch 67.50 | loss 47.66 | mse 47.41 | mre 1482156.60 |\n",
      "scGPT - INFO - | epoch  14 | 600/675 batches | lr 0.0000 | ms/batch 67.49 | loss 47.56 | mse 47.31 | mre 1484558.68 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 47.67s | valid loss/mse 48.5540 | mre 1520778.7397\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/675 batches | lr 0.0000 | ms/batch 68.02 | loss 49.32 | mse 49.07 | mre 1538373.43 |\n",
      "scGPT - INFO - | epoch  15 | 200/675 batches | lr 0.0000 | ms/batch 67.51 | loss 48.93 | mse 48.68 | mre 1513470.39 |\n",
      "scGPT - INFO - | epoch  15 | 300/675 batches | lr 0.0000 | ms/batch 67.24 | loss 49.43 | mse 49.17 | mre 1519997.43 |\n",
      "scGPT - INFO - | epoch  15 | 400/675 batches | lr 0.0000 | ms/batch 67.36 | loss 48.62 | mse 48.37 | mre 1515393.36 |\n",
      "scGPT - INFO - | epoch  15 | 500/675 batches | lr 0.0000 | ms/batch 67.59 | loss 47.41 | mse 47.16 | mre 1475042.03 |\n",
      "scGPT - INFO - | epoch  15 | 600/675 batches | lr 0.0000 | ms/batch 67.70 | loss 47.26 | mse 47.01 | mre 1479866.02 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 47.59s | valid loss/mse 48.1832 | mre 1494844.1754\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 48.1832\n",
      "scGPT - INFO - Saving model to save/dev_PBMC_10K-Jul04-17-32\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "#         # eval on testdata\n",
    "#         results = eval_testdata(\n",
    "#             best_model,\n",
    "#             adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "#             include_types=[\"cls\"],\n",
    "#         )\n",
    "#         results[\"batch_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "\n",
    "#         results[\"celltype_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "#         metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "#         metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "#             str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "#             caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "#         )\n",
    "\n",
    "#         metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "#             str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "#             caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "#         )\n",
    "#         metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "#         wandb.log(metrics_to_log)\n",
    "#         wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2914: ResourceWarning: unclosed <ssl.SSLSocket fd=135, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.10.10.161', 44334), raddr=('35.186.228.49', 443)>\n",
      "  self._assert_can_log_artifact(artifact)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/mse</td><td>█▅▃▄▃▃▃▃▃▂▂▂▃▃▃▄▃▃▃▃▂▃▄▂▃▂▃▄▁▂▂▂▂▂▃▂▂▂▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▅▅▄▃▃▃▃▂▃▃▃▃▃▄▃▃▃▄▂▃▄▂▃▂▃▄▁▂▂▃▂▂▃▂▃▂▂▂</td></tr><tr><td>valid/mre</td><td>▇▄▇▅▇█▂▆▁▅▂▃▄▄▃</td></tr><tr><td>valid/mse</td><td>█▆▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/mse</td><td>53.91365</td></tr><tr><td>train/nzlp</td><td>0.26376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-disco-8</strong> at: <a href='https://wandb.ai/qiliu-ghddi/scGPT/runs/p6q7wsnl' target=\"_blank\">https://wandb.ai/qiliu-ghddi/scGPT/runs/p6q7wsnl</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230704_173251-p6q7wsnl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "single_cell_gpt",
   "language": "python",
   "name": "single_cell_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
