{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2f17dc-4ec5-4168-aafc-757fe0c84f2e",
   "metadata": {},
   "source": [
    "# Pretraining on the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ea2c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3790c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libjpeg.so.8: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import issparse\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import Vocab as VocabPybind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae20f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_on_cre_all_in_one/notebook/../scgpt/model/model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/qiliu02/miniconda3/envs/single_cell_gpt/lib/python3.9/site-packages/scanpy/_settings.py:447: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, \"../\")\n",
    "from scgpt.utils import set_seed\n",
    "from scgpt.utils import category_str2int, eval_scib_metrics\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "import scgpt as scg\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0603a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\".\")\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "from config import hyperparameter_defaults\n",
    "from pretrain_all_in_one_0801 import load_tokenized, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f3d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 790 ms, total: 790 ms\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dataset_name=\"cre329_tokenized_merged_numds10\"\n",
    "# file_path=\"/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_on_cre_all_in_one/script/save/cre329_tokenized_merged/20230801_175734/cre329_tokenized_merged_numds10.pt\"\n",
    "\n",
    "dataset_name=\"cre329_tokenized_merged_numds2\"\n",
    "file_path=\"/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_on_cre_all_in_one/script/save/cre329_tokenized_merged/20230803_104338/cre329_tokenized_merged_numds2.pt\"\n",
    "\n",
    "tokenized_data = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fade1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genes': tensor([[60695,  3213,  3832,  ..., 12229, 19237, 35831],\n",
       "         [60695,  2520, 35271,  ..., 19445, 19851, 33698],\n",
       "         [60695,  1955,  3291,  ...,  3434, 18244, 20874],\n",
       "         ...,\n",
       "         [60695, 17577, 36641,  ..., 60694, 60694, 60694],\n",
       "         [60695, 32624,  3330,  ..., 16400, 20098,  7593],\n",
       "         [60695, 11107,  1495,  ..., 33851, 20059,  4986]]),\n",
       " 'values': tensor([[ 0, 15,  1,  ..., 28, 50, 49],\n",
       "         [ 0, 10, 11,  ..., 15, 11, 45],\n",
       "         [ 0,  3,  8,  ..., 46, 44,  9],\n",
       "         ...,\n",
       "         [ 0, 37, 45,  ..., -2, -2, -2],\n",
       "         [ 0, 18,  9,  ...,  7, 17, 33],\n",
       "         [ 0,  3, 13,  ..., 49, 41, 17]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebafe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60695,  3213,  3832,  ..., 12229, 19237, 35831],\n",
       "        [60695,  2520, 35271,  ..., 19445, 19851, 33698],\n",
       "        [60695,  1955,  3291,  ...,  3434, 18244, 20874],\n",
       "        ...,\n",
       "        [60695, 17577, 36641,  ..., 60694, 60694, 60694],\n",
       "        [60695, 32624,  3330,  ..., 16400, 20098,  7593],\n",
       "        [60695, 11107,  1495,  ..., 33851, 20059,  4986]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['genes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b2c0a9-26be-47f2-b467-205cd613cd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20358, 1201])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['genes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9306d711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 15,  1,  ..., 28, 50, 49],\n",
       "        [ 0, 10, 11,  ..., 15, 11, 45],\n",
       "        [ 0,  3,  8,  ..., 46, 44,  9],\n",
       "        ...,\n",
       "        [ 0, 37, 45,  ..., -2, -2, -2],\n",
       "        [ 0, 18,  9,  ...,  7, 17, 33],\n",
       "        [ 0,  3, 13,  ..., 49, 41, 17]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439aaae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genes_train: 19747\n",
      "genes_valid: 611\n",
      "values_train: 19747\n",
      "values_valid: 611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genes_train, genes_valid, values_train, values_valid = train_test_split(\n",
    "    tokenized_data['genes'], \n",
    "    tokenized_data['values'],\n",
    "    test_size=0.8,\n",
    "    shuffle=False\n",
    ")\n",
    "print(f\"genes_train: {len(genes_train)}\")\n",
    "print(f\"genes_valid: {len(genes_valid)}\")\n",
    "print(f\"values_train: {len(values_train)}\")\n",
    "print(f\"values_valid: {len(values_valid)}\")\n",
    "\n",
    "\n",
    "tokenized_train = {\n",
    "    'genes': genes_train,\n",
    "    'values': values_train\n",
    "}\n",
    "\n",
    "tokenized_valid = {\n",
    "    'genes': genes_valid,\n",
    "    'values': values_valid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bff032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60695,  3213,  3832,  ..., 12229, 19237, 35831],\n",
       "        [60695,  2520, 35271,  ..., 19445, 19851, 33698],\n",
       "        [60695,  1955,  3291,  ...,  3434, 18244, 20874],\n",
       "        ...,\n",
       "        [60695, 12118, 17632,  ..., 19488,  8714, 18912],\n",
       "        [60695,  9242,  8714,  ..., 19090, 16134, 16244],\n",
       "        [60695,  4533, 33092,  ...,  8591,  2106, 12087]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beacd9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60695,  1418, 19185,  ..., 21054, 20678, 10964],\n",
       "        [60695, 17714,  5291,  ..., 33470, 35192, 32859],\n",
       "        [60695, 32162, 17324,  ..., 36550, 19433,  8251],\n",
       "        ...,\n",
       "        [60695, 17577, 36641,  ..., 60694, 60694, 60694],\n",
       "        [60695, 32624,  3330,  ..., 16400, 20098,  7593],\n",
       "        [60695, 11107,  1495,  ..., 33851, 20059,  4986]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7920820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train, tokenized_valid = load_tokenized(\n",
    "#     dataset_name=\"cre329_tokenized_merged_numds10\",\n",
    "#     file_path=\"/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_on_cre_all_in_one/script/save/cre329_tokenized_merged/20230801_175734/cre329_tokenized_merged_numds10.pt\",\n",
    "#     valid_size=0.03\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7068781",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a40d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "if do_train:\n",
    "    config = {\n",
    "        'seed': 42,\n",
    "        'dataset_name': \"cre329_tokenized_merged_numds10\",\n",
    "        'dataset_filepath': \"../data/cre329_tokenized_merged/cre329_tokenized_merged_numds10.pt\",\n",
    "        \n",
    "        'do_train': True,\n",
    "        'load_model': None,\n",
    "\n",
    "        'layer_size': 128,\n",
    "        'nhead': 4,\n",
    "        'nlayers': 4,\n",
    "\n",
    "        'n_hvg': None,\n",
    "        'max_seq_len': 1200,\n",
    "\n",
    "        'fast_transformer': False,\n",
    "        'pre_norm': False,\n",
    "        'ecs_thres': 0.8,\n",
    "\n",
    "        'GEPC': False,\n",
    "        'DSBN': False,\n",
    "\n",
    "        'batch_size': 32,\n",
    "        'epochs': 6,\n",
    "        'lr': 1e-4,\n",
    "        \"dropout\": 0.2,\n",
    "\n",
    "        'n_bins': 51,\n",
    "        'mask_ratio': 0.4,\n",
    "\n",
    "        'amp': False,\n",
    "        'schedule_ratio': 0.9,\n",
    "        'save_eval_interval': 5,\n",
    "        'log_interval': 100,\n",
    "    }\n",
    "    embsize = config['layer_size']\n",
    "    d_hid = config['layer_size']\n",
    "    nhead = config['nhead']\n",
    "    nlayers = config['nlayers']\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    dropout = config['dropout']\n",
    "    pre_norm = config['pre_norm']\n",
    "    ecs_thres = config['ecs_thres']\n",
    "\n",
    "    n_hvg = None\n",
    "    max_seq_len = 1201\n",
    "\n",
    "    DSBN = False\n",
    "    GEPC = False\n",
    "    do_dab = False\n",
    "    use_batch_labels = False\n",
    "    num_batch_types = None\n",
    "\n",
    "    per_seq_batch_sample = False\n",
    "    explicit_zero_prob = False  # whether explicit bernoulli for zeros\n",
    "else:\n",
    "    config = {\n",
    "        'seed': 42,\n",
    "        'dataset_name': \"PBMC_10K\",\n",
    "        'do_train': True,\n",
    "        'load_model': \"./save/scGPT_human\",\n",
    "        'GEPC': True,\n",
    "        'ecs_thres': 0.8,\n",
    "        'dab_weight': 1.0,\n",
    "        'mask_ratio': 0.4,\n",
    "        'epochs': 15,\n",
    "        'n_bins': 51,\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': 16,\n",
    "\n",
    "        'layer_size': 128,\n",
    "        'nlayers': 4,\n",
    "        'nhead': 4,\n",
    "\n",
    "        'dropout': 0.2,\n",
    "        'schedule_ratio': 0.9,\n",
    "        'save_eval_interval': 5,\n",
    "        'log_interval': 100,\n",
    "\n",
    "        'fast_transformer': False,\n",
    "        'pre_norm': False,\n",
    "        'amp': True,\n",
    "    }\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4ebf148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqiliu-ghddi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qiliu02/GHDDI/DS-group/ghddixcre_singlecell_gpt/dev_pretrain_on_cre_all_in_one/notebook/wandb/run-20230803_104431-h2wj495n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT/runs/h2wj495n' target=\"_blank\">wise-terrain-7</a></strong> to <a href='https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT' target=\"_blank\">https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT/runs/h2wj495n' target=\"_blank\">https://wandb.ai/qiliu-ghddi/Pretraining%20scGPT/runs/h2wj495n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'cre329_tokenized_merged_numds10', 'dataset_filepath': '../data/cre329_tokenized_merged/cre329_tokenized_merged_numds10.pt', 'do_train': True, 'load_model': None, 'layer_size': 128, 'nhead': 4, 'nlayers': 4, 'n_hvg': None, 'max_seq_len': 1200, 'fast_transformer': False, 'pre_norm': False, 'ecs_thres': 0.8, 'GEPC': False, 'DSBN': False, 'batch_size': 32, 'epochs': 6, 'lr': 0.0001, 'dropout': 0.2, 'n_bins': 51, 'mask_ratio': 0.4, 'amp': False, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "dataset_name = \"cre329_tokenized_merged_numds10\"  # config['dataset_name']\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=config,\n",
    "    project=\"Pretraining scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "set_seed(config.seed)\n",
    "print(config)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af10016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - ntokens 60697\n",
      "ntokens 60697\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "mask_ratio = config['mask_ratio']\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config['n_bins']\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "vocab_file = \"default_census_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s) \n",
    "ntokens = len(vocab)  # size of vocabulary, 60694\n",
    "\n",
    "logger.info(f\"ntokens {ntokens}\")\n",
    "print(f\"ntokens {ntokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20d3ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple batchnorm instead of domain specific batchnorm\n"
     ]
    }
   ],
   "source": [
    "# training    \n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=None,\n",
    "    domain_spec_batchnorm=False,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=ecs_thres,\n",
    "    explicit_zero_prob=True,\n",
    "    use_fast_transformer=False,\n",
    "    pre_norm=pre_norm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0544224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e0a5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain_all_in_one_0801 import define_wandb_metrcis\n",
    "\n",
    "lr = config['lr']\n",
    "eps = 1e-4 if config['amp'] else 1e-8\n",
    "criterion = masked_mse_loss\n",
    "# criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=lr,\n",
    "    eps=eps\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    1, \n",
    "    gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "\n",
    "## Step 4: Finetune scGPT with task-specific objectives\n",
    "best_val_loss = float(\"inf\")\n",
    "# best_avg_bio = 0.0\n",
    "\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "sort_seq_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2eebe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'cre329_tokenized_merged_numds10', 'dataset_filepath': '../data/cre329_tokenized_merged/cre329_tokenized_merged_numds10.pt', 'do_train': True, 'load_model': None, 'layer_size': 128, 'nhead': 4, 'nlayers': 4, 'n_hvg': None, 'max_seq_len': 1200, 'fast_transformer': False, 'pre_norm': False, 'ecs_thres': 0.8, 'GEPC': False, 'DSBN': False, 'batch_size': 32, 'epochs': 6, 'lr': 0.0001, 'dropout': 0.2, 'n_bins': 51, 'mask_ratio': 0.4, 'amp': False, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3188fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain_all_in_one_0801 import train, evaluate, prepare_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286ab497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   1 | 100/618 batches | lr 0.0001 | ms/batch 350.97 | loss 733.94 | mse 733.94 | mre 2151.12 |\n",
      "scGPT - INFO - | epoch   1 | 200/618 batches | lr 0.0001 | ms/batch 252.12 | loss 363.45 | mse 363.45 | mre 11535.80 |\n",
      "scGPT - INFO - | epoch   1 | 300/618 batches | lr 0.0001 | ms/batch 255.45 | loss 203.13 | mse 203.13 | mre 18604.52 |\n",
      "scGPT - INFO - | epoch   1 | 400/618 batches | lr 0.0001 | ms/batch 264.96 | loss 188.15 | mse 188.15 | mre 13388.81 |\n",
      "scGPT - INFO - | epoch   1 | 500/618 batches | lr 0.0001 | ms/batch 254.44 | loss 182.55 | mse 182.55 | mre 6651.91 |\n",
      "scGPT - INFO - | epoch   1 | 600/618 batches | lr 0.0001 | ms/batch 253.04 | loss 188.15 | mse 188.15 | mre 4009.97 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 174.60s | valid loss/mse 191.4336 | mre 321.9247\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 191.4336\n",
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   2 | 100/618 batches | lr 0.0001 | ms/batch 251.85 | loss 194.30 | mse 194.30 | mre 928.34 |\n",
      "scGPT - INFO - | epoch   2 | 200/618 batches | lr 0.0001 | ms/batch 261.00 | loss 191.52 | mse 191.52 | mre 494.44 |\n",
      "scGPT - INFO - | epoch   2 | 300/618 batches | lr 0.0001 | ms/batch 244.13 | loss 186.95 | mse 186.95 | mre 321.90 |\n",
      "scGPT - INFO - | epoch   2 | 400/618 batches | lr 0.0001 | ms/batch 258.12 | loss 181.49 | mse 181.49 | mre 233.59 |\n",
      "scGPT - INFO - | epoch   2 | 500/618 batches | lr 0.0001 | ms/batch 260.35 | loss 179.71 | mse 179.71 | mre 226.10 |\n",
      "scGPT - INFO - | epoch   2 | 600/618 batches | lr 0.0001 | ms/batch 270.91 | loss 184.17 | mse 184.17 | mre 460.18 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 166.96s | valid loss/mse 182.5775 | mre 248.9284\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 182.5775\n",
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   3 | 100/618 batches | lr 0.0001 | ms/batch 272.66 | loss 193.73 | mse 193.73 | mre 879.24 |\n",
      "scGPT - INFO - | epoch   3 | 200/618 batches | lr 0.0001 | ms/batch 273.33 | loss 191.02 | mse 191.02 | mre 365.52 |\n",
      "scGPT - INFO - | epoch   3 | 300/618 batches | lr 0.0001 | ms/batch 277.14 | loss 186.42 | mse 186.42 | mre 283.91 |\n",
      "scGPT - INFO - | epoch   3 | 400/618 batches | lr 0.0001 | ms/batch 280.50 | loss 181.24 | mse 181.24 | mre 235.55 |\n",
      "scGPT - INFO - | epoch   3 | 500/618 batches | lr 0.0001 | ms/batch 269.55 | loss 179.29 | mse 179.29 | mre 209.95 |\n",
      "scGPT - INFO - | epoch   3 | 600/618 batches | lr 0.0001 | ms/batch 267.41 | loss 182.28 | mse 182.28 | mre 189.75 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 175.77s | valid loss/mse 174.3874 | mre 253.6717\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 174.3874\n",
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   4 | 100/618 batches | lr 0.0001 | ms/batch 259.85 | loss 194.19 | mse 194.19 | mre 406.08 |\n",
      "scGPT - INFO - | epoch   4 | 200/618 batches | lr 0.0001 | ms/batch 259.53 | loss 191.04 | mse 191.04 | mre 351.71 |\n",
      "scGPT - INFO - | epoch   4 | 300/618 batches | lr 0.0001 | ms/batch 257.58 | loss 186.30 | mse 186.30 | mre 277.20 |\n",
      "scGPT - INFO - | epoch   4 | 400/618 batches | lr 0.0001 | ms/batch 258.03 | loss 181.11 | mse 181.11 | mre 167.38 |\n",
      "scGPT - INFO - | epoch   4 | 500/618 batches | lr 0.0001 | ms/batch 259.76 | loss 179.22 | mse 179.22 | mre 155.74 |\n",
      "scGPT - INFO - | epoch   4 | 600/618 batches | lr 0.0001 | ms/batch 256.68 | loss 181.29 | mse 181.29 | mre 133.03 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 166.81s | valid loss/mse 166.4506 | mre 145.3914\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 166.4506\n",
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   5 | 100/618 batches | lr 0.0001 | ms/batch 259.21 | loss 194.44 | mse 194.44 | mre 217.96 |\n",
      "scGPT - INFO - | epoch   5 | 200/618 batches | lr 0.0001 | ms/batch 283.78 | loss 190.86 | mse 190.86 | mre 282.60 |\n",
      "scGPT - INFO - | epoch   5 | 300/618 batches | lr 0.0001 | ms/batch 251.36 | loss 186.35 | mse 186.35 | mre 214.96 |\n",
      "scGPT - INFO - | epoch   5 | 400/618 batches | lr 0.0001 | ms/batch 262.21 | loss 180.78 | mse 180.78 | mre 127.59 |\n",
      "scGPT - INFO - | epoch   5 | 500/618 batches | lr 0.0001 | ms/batch 255.05 | loss 179.16 | mse 179.16 | mre 113.38 |\n",
      "scGPT - INFO - | epoch   5 | 600/618 batches | lr 0.0001 | ms/batch 264.40 | loss 182.00 | mse 182.00 | mre 109.67 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 168.49s | valid loss/mse 171.3753 | mre 440.3039\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_cre329_tokenized_merged_numds10-Aug03-10-44\n",
      "random masking at epoch , ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - ===\n",
      "scGPT - INFO - config.do_train\n",
      "scGPT - INFO - | epoch   6 | 100/618 batches | lr 0.0001 | ms/batch 264.45 | loss 195.28 | mse 195.28 | mre 290.68 |\n",
      "scGPT - INFO - | epoch   6 | 200/618 batches | lr 0.0001 | ms/batch 257.08 | loss 190.81 | mse 190.81 | mre 138.72 |\n",
      "scGPT - INFO - | epoch   6 | 300/618 batches | lr 0.0001 | ms/batch 266.33 | loss 186.39 | mse 186.39 | mre 129.63 |\n",
      "scGPT - INFO - | epoch   6 | 400/618 batches | lr 0.0001 | ms/batch 264.98 | loss 180.62 | mse 180.62 | mre 82.31 |\n",
      "scGPT - INFO - | epoch   6 | 500/618 batches | lr 0.0001 | ms/batch 277.84 | loss 178.88 | mse 178.88 | mre 77.72 |\n",
      "scGPT - INFO - | epoch   6 | 600/618 batches | lr 0.0001 | ms/batch 259.41 | loss 181.84 | mse 181.84 | mre 65.81 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 170.35s | valid loss/mse 171.0408 | mre 54.9934\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Saving model to save/dev_cre329_tokenized_merged_numds10-Aug03-10-44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2b41f32de400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_data_pt, valid_data_pt = prepare_data(\n",
    "        tokenized_train,\n",
    "        tokenized_valid,\n",
    "        mask_ratio,\n",
    "        mask_value,\n",
    "        pad_value\n",
    "    )\n",
    "\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    logger.info(\"===\")\n",
    "    if do_train:\n",
    "        logger.info(\"config.do_train\")\n",
    "        # training one epoch\n",
    "        train(\n",
    "            config,\n",
    "            epoch,\n",
    "            model,\n",
    "            device,\n",
    "            loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            scheduler=scheduler,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "    # evaluation of one epoch\n",
    "    val_loss, val_mre = evaluate(\n",
    "        config,\n",
    "        epoch,\n",
    "        model,\n",
    "        device,\n",
    "        loader=valid_loader,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "#         # eval on testdata\n",
    "#         results = eval_testdata(\n",
    "#             best_model,\n",
    "#             adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "#             include_types=[\"cls\"],\n",
    "#         )\n",
    "#         results[\"batch_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "\n",
    "#         results[\"celltype_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "#         metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "#         metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "#             str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "#             caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "#         )\n",
    "\n",
    "#         metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "#             str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "#             caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "#         )\n",
    "#         metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "#         wandb.log(metrics_to_log)\n",
    "#         wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n",
    "\n",
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef65f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "single_cell_gpt",
   "language": "python",
   "name": "single_cell_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
